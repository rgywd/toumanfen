# 模型配置

baseline:
  # 随机森林 + TF-IDF 配置
  random_forest:
    n_estimators: 100
    max_depth: 50
    min_samples_split: 2
    min_samples_leaf: 1
    n_jobs: -1
    random_state: 42
    
  # TF-IDF 配置
  tfidf:
    max_features: 10000
    ngram_range: [1, 2]
    min_df: 2
    max_df: 0.95
    sublinear_tf: true
    
  # FastText 配置
  fasttext:
    dim: 100
    epoch: 25
    lr: 0.5
    wordNgrams: 2
    minCount: 2
    loss: "softmax"
    bucket: 2000000

bert:
  # BERT 模型配置
  model_name: "hfl/chinese-bert-wwm-ext"
  num_labels: 10
  max_length: 128
  hidden_dropout_prob: 0.1
  attention_probs_dropout_prob: 0.1
  classifier_dropout: 0.1
  
  # 可选的其他中文BERT模型
  alternative_models:
    - "bert-base-chinese"
    - "hfl/chinese-roberta-wwm-ext"
    - "hfl/chinese-macbert-base"

optimization:
  # 量化配置
  quantization:
    dtype: "int8"
    dynamic: true
    
  # 剪枝配置
  pruning:
    sparsity: 0.5
    method: "magnitude"
    structured: false
    
  # 蒸馏配置
  distillation:
    temperature: 4.0
    alpha: 0.5
    student_model: "hfl/chinese-bert-wwm-ext"
